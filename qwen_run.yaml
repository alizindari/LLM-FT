### model
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct

### method
stage: sft
do_train: true
finetuning_type: full
lora_rank: 256
lora_target: all

### dataset
dataset: gsm8k               # GSM8K (main split) configured in dataset_info.json
template: qwen
cutoff_len: 2048
max_samples: null            # Use the full training split
val_size: 0.2                # Reserve 20% for validation
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: saves/qwen-1.5b-real/gsm8k/full
overwrite_output_dir: true
plot_loss: true              # <--- This generates loss.png (Train & Val) automatically

### logging & evaluation control
logging_steps: 5             # Print training loss every 5 steps
eval_steps: 5               # Calculate validation loss every 20 steps
save_steps: 20               # Save checkpoint every 20 steps
eval_strategy: steps         # specifices we want to eval by steps, not epochs
do_eval: true                # vital: enables validation calculation

### train hyperparameters (Tuned for 4x A100)
per_device_train_batch_size: 4    # Keep micro-batch tiny to avoid OOM
gradient_accumulation_steps: 8    # Restores effective global batch size (4*4*8=128)
per_device_eval_batch_size: 2
learning_rate: 1e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

### distributed
deepspeed: examples/deepspeed/ds_z2_config.json